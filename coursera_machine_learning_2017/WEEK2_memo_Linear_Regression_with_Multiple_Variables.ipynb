{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK2 Linear Regression with Multiple Variables\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple variate linear regression\n",
    "\n",
    "- $ x^{(i)}$: $i$ th data vector\n",
    "- $ x^{(i)}_j$: $j$ th feature of $i$ th data vector\n",
    "- $x_0$ often use as a bias. (Set $ x_0 = 1$ then $ \\theta_0 x_1 = \\theta_0$.)\n",
    "\n",
    "\n",
    "### hypothesis\n",
    "\n",
    "$ h(x) = \\theta^Tx$\n",
    "\n",
    "\n",
    "### cost function\n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h(x^{(i)}) - y^{(i)} \\right)^2$\n",
    "\n",
    "\n",
    "### Learning: Gradient descent\n",
    "\n",
    "repeat {\n",
    "\n",
    "$\\theta_j := \\theta_j -\\alpha J(\\theta)\\Delta\\theta_j$\n",
    "\n",
    "}\n",
    "\n",
    "that is,\n",
    "\n",
    "$\\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1} ^m \\left(h(x^{(i)}) - y^{(i)}\\right) x_j^{(i)}$\n",
    "\n",
    "各特徴量の方向に対して、答えとの差を見て、その傾きに応じて反対向きに傾きを変えている。\n",
    "\n",
    "つまり、特徴量が互いに垂直なベクトル上の変数である場合には確実に収束する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent in practice\n",
    "特徴量同士のスケールは合わせなければ、ある特定の方向の学習に時間がかかってしまう\n",
    "\n",
    "-1から1にスケールするのが良いが、ある程度スケールが合っている程度で良い\n",
    "\n",
    "### Feature scaling\n",
    "\n",
    "$x_i$ を(最大値-最小値)で割る ($x_0=1$には適用しない)\n",
    "\n",
    "### Mean normalization\n",
    "$x_i$ を、$x_i - μ_i$ に置き換える ($x_0=1$には適用しない)\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "- Gradient descentが正しく働いている方法かどうか見極める方法:\n",
    "\n",
    "  plot cost function $J(\\theta)$: $x$ axis is iterations → should decrease after every iteration\n",
    "\n",
    "  $J(\\theta)$の値が、あるイテレーションで$10^{-3}$など、非常に変化量でしか小さくなっていかなくなったとき、収束したとする。\n",
    "\n",
    "  もちろんこの$10^{-3}$ という値は問題によって異なるため、plotを見て判断するべき。\n",
    "\n",
    "\n",
    "- Learning rate $\\alpha$の選び方\n",
    "\n",
    "  $J(\\theta)$が上昇しているときはglobal minimumを飛び越えて振動しているので、もっと$\\alpha$を小さくするべき\n",
    "\n",
    "  $J(\\theta)$が小さくなって大きくなってを繰り返すときも同様\n",
    "\n",
    "  $\\alpha$の例: $0.001, 0.01, 0.1, 1, \\dots$ で試してみる → それから細かい値を試していく\n",
    "\n",
    "### Features\n",
    "与えられた変数から新たな変数を作れる場合は作ってみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "モデルとする多項式に十分な表現力が無い場合は、二次、三次、...の多項式にして各変数に適用しよう\n",
    "\n",
    "多変数の場合はどうなる？ → モデルが $ y = a_1 x + a_2 x^2$ の場合、$ y = a_1 ( \\theta_1 x + \\theta_2 x + \\theta_3 x) + a_1 ( \\theta_1 x + \\theta_2 x + \\theta_3 x)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation\n",
    "大域最適解を解析的に解くやり方。\n",
    "\n",
    "オーダーは$ O(n^3)$ なので、特徴量の数$n$が$1000$とかを超えて$10000$とかになると厳しくなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
